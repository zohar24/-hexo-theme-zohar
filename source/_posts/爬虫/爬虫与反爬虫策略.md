---
title: 爬虫与发爬虫策略
author: zohar
top: false
cover: false
toc: false
date: 2022-12-27 18:45:13
tags:
- 爬虫
- 策略
categories:
- 爬虫
---

# 什么是爬虫
爬虫是一个模拟人类请求网站行为的程序。可以自动请求网页、并将数据抓取下来，然后使用一定的规则提取有价值的数据。说白了就是使用任何技术手段，批量获取网站信息的一种方式。

# 爬虫是正经爬虫流不流氓我不知道

爬虫需要遵守法律，程序员不仅要学会保护好自己头发还有自己。

Robots 协议（也称爬虫协议，机器人协议）是互联网爬虫的一项公认的道德规范，全称是“网络爬虫排除标准（Robots exclusion protocol）”，这个协议用来告诉引擎，哪些页面是可以抓取的，哪些不可以，常见的查看robots协议的方法是在网址后+“/robots.txt’。也可以使用 [在线监测robots协议工具](http://www.wetools.com/robots-tester) 查看。

# 技术
爬虫技术这里就不多说了，我们这里多聊聊策略，想了解爬虫框架这里可以参考一下 [主流爬虫框架比对](http://zohar24.github.io/2022/12/17/爬虫/主流爬虫框架对比/)。



# 爬虫策略

1. 深度优先遍历策略

	深度优先遍历策略是指网络爬虫会从起始页开始，一个链接一个链接跟踪下去，处理完这条线路的链接之后，在再转入下一个起始页，继续跟踪链接。

2. 广度优先遍历策略

	广度优先策略是按照树的层次进行搜索，如果此层没有搜索完成，不会进入下一层搜索。即首先完成一个层次的搜索，其次在进行下一层次，也称之为分层处理。

3. 部分PageRank的策略

	PageRank算法的思想：对于已经下载的网页，连同待抓取URL队列的URL，形成网页集合，计算每个页面的PageRank值（PageRank算法参考：PageRank算法 - 从原理到实现），计算完之后，将待抓取队列中的URL按照网页级别的值的大小排列，并按照顺序依次抓取网址页面。

4. OPIC策略策略

	基本思路：在算法开始前，给所有页面一个相同的初始现金（现金）当下载了某个页面P之后，将P的现金分摊给所有从P中分析出的链接，并且将P的现金清空。对于待抓取URL队列中的所有页面按照现金数进行排序。
与PageRank的的的的区别在于：PageRank的的的每次需要迭代计算，而OPIC策略不需要迭代过程所以计算速度远远快与PageRank的的的，适合实时计算使用。

5. 大站优先策略
	
	以网站为单位来选题网页重要性，对于待爬取URL队列中的网页，根据所属网站归类，如果哪个网站等待下载的页面最多，则优先下载这些链接，其本质思想倾向于优先下载大型网站。因为大型网站往往包含更多的页面。鉴于大型网站往往是著名企业的内容，其网页质量一般较高，所以这个思路虽然简单，但是有一定依据。实验表明这个算法效果也要略优先于宽度优先遍历策略。

# 反爬虫策略

1. 基于IP反爬虫(IP限制频次)

	日志进行分析时有时会发现同一时间段内某一个或某几个IP访问量特别大，由于爬虫是通过程序来自动化爬取页面信息的，因此其单位时间的请求量较大，且相邻请求时间间隔较为固定，这时就基本可以判断此类行为系爬虫所为，此时即可在服务器上对异常IP进行封锁。

2. 通过Header反爬虫(User-Agent + Referer检测)

	User-Agent是请求头域之一，服务器从User-Agent对应的值中是被客户端的使用信息。
	User-Agent的角色就是客户端的身份标识。很多的爬虫请求头就是默认的一些很明显的爬虫头python-requests/2.18.4，诸如此类，当发现携带有这类headers的数据包，直接拒绝访问，返回403错误。
	除了User-Agent之外，可利用的头域还有Host和Referer。这种验证请求头信息中特定头域的方式既可以有效地屏蔽长期无人维护的爬虫程序，也可以将一些爬虫初学者发出的网络请求拒之门外。

3. 基于用户行为反爬虫

	日志进行分析时用户在长时间内循环某一个操作，且相邻请求时间间隔较为固定，这时就基本可以判断此类行为系爬虫所为。此时即可在服务器上对异常IP和用户进行封锁。

4. iframe嵌入

	核心数据展示在iframe标签内，能够动态扩展页面，重要的是可根据逻辑动态改变页面内容。

5. Cookie限制(登录限制)

	Cookie限制指的是服务器通过校验请求头中的Cookie值来区分正常用户和爬虫程序的手段，服务器对每一个访问网页的人都会给其一个Cookie，有的扫描爬虫单纯为了爬取链接，并不会对Cookie进行处理和响应。
	当某个Cookie访问超过某一个阀值时，就对其进行封禁，过一段时间再放出来。也可以把Cookie和JavaScript结合起来实现反爬虫从而提高爬虫难度，这种手段被广泛应用在Web应用中。

6. 验证码限制

	当某一用户访问次数过多后，就自动让请求跳转到一个验证码页面，只有在输入正确的验证码或指定操作之后才能继续访问网站。此限制使用最为广泛。

7. js加密(通过js加密请求参数、解密加密的数据来反爬)

	由 JavaScript 改变 HTML DOM 导致页面内容发生变化的现象称为动态渲染。
	由于编程语言没有像浏览器一样内置JavaScript解释器和渲染引擎，所以动态渲染是天然的反爬虫手段。
	网页开发者将重要信息放在网页中但不写入html标签中，而浏览器会自动渲染`<script>`标签中的js代码将信息展现在浏览器当中，而爬虫是不具备执行js代码的能力，所以无法将js事件产生的信息读取出来。

8. CSS偏移反爬虫(CSS偏移反爬虫指的是利用CSS样式将乱序的文字排版为人类正常阅读的顺序)。

	这种方法是利用 CSS 样式将乱序的文字排版为人类正常阅读顺序的行为。
	如果不细心观察，爬虫工程师很容易被爬取结果糊弄。这种混淆方法和图片伪装一样，并不会影响用户阅读。

# 总结

反爬虫系统最终总是能破解，只是时间问题。这句话是什么意思呢？其实这句话你可以这么理解：反爬虫系统，对于爬虫来说，相当于一个单机游戏。单机游戏，总有通关的一天。也许是明天，也许是后天，也许是下个月、半年？但是总是过得去的。甚至你可以上上下下左右左右BABA嘛。那么什么游戏不能通关呢？没错，网络游戏不能通关，它可以无休止地升级下去。
你装备刚毕业，他改版了。就这样，一直下去，等于没有破解。就像一些模拟战争游戏抢要塞一样，刚抢完没多久，过期了，又要重抢。这就等于没怎么抢到手。